In this project, Ollama has been used to run any modal (which ollama supports) locally on system as inference server. For this example, I have used mistral model.
Here I am trying to perform some operation on prodided file.
